import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# ------------------------------------------------------------------------
# 1. ì„¤ì • (Configuration)
# ------------------------------------------------------------------------
# ì‚¬ìš©í•˜ë ¤ëŠ” ëª¨ë¸ ID (Gemma-2-2b-it ë˜ëŠ” Llama-3.2-1B-Instruct ì¶”ì²œ)
# ë§Œì•½ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œ ë°›ì€ ê²½ë¡œê°€ ìˆë‹¤ë©´ ê·¸ ê²½ë¡œë¥¼ ì ì–´ì£¼ì„¸ìš”.
MODEL_ID = "google/gemma-3-1b-it" 

print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘... ({MODEL_ID})")

# ------------------------------------------------------------------------
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (4-bit Quantization ì ìš©)
# ------------------------------------------------------------------------
# ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",           # 4-bit NormalFloat (ì„±ëŠ¥ ìœ ì§€ì— ìœ ë¦¬)
    bnb_4bit_compute_dtype=torch.float16 # ì—°ì‚°ì€ fp16ìœ¼ë¡œ ìˆ˜í–‰
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto" # GPU ìë™ í• ë‹¹
)

# íŒŒì´í”„ë¼ì¸ ìƒì„± (í…ìŠ¤íŠ¸ ìƒì„±ì„ ì‰½ê²Œ í•˜ê¸° ìœ„í•¨)
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n")

# ------------------------------------------------------------------------
# 3. í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (SFT í•„ìš” ì—¬ë¶€ íŒë‹¨ìš©)
# ------------------------------------------------------------------------
# ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ê°€ì‚¬ë¥¼ ìš”ì²­í•´ë´…ë‹ˆë‹¤.
test_prompts = [
    {
        "role": "user", 
        "content": "ë„ˆëŠ” í•œêµ­ì–´ ì‘ì‚¬ê°€ì•¼. 'í—¤ì–´ì§„ ì—°ì¸ì„ ê·¸ë¦¬ì›Œí•˜ëŠ” ë°¤'ì„ ì£¼ì œë¡œ ê°ì„±ì ì¸ ë°œë¼ë“œ ê°€ì‚¬ë¥¼ ì¨ì¤˜. (Verse 1 - Chorus êµ¬ì¡°ë¡œ)"
    },
    {
        "role": "user", 
        "content": "ë„ˆëŠ” í™í•© ì‘ì‚¬ê°€ì•¼. 'ì„±ê³µì„ í–¥í•œ ì—´ì •'ì„ ì£¼ì œë¡œ ë¼ì„(Rhyme)ì„ ì‚´ë ¤ì„œ ë© ê°€ì‚¬ë¥¼ ì¨ì¤˜."
    }
]

# ------------------------------------------------------------------------
# 4. ì¶”ë¡  ë° ê²°ê³¼ ì¶œë ¥
# ------------------------------------------------------------------------
for i, msg in enumerate(test_prompts):
    print(f"--- [Test Case {i+1}] ---")
    print(f"ì£¼ì œ: {msg['content']}")
    
    # Gemma/Llamaì˜ ì±„íŒ… í…œí”Œë¦¿ ì ìš©
    messages = [msg]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    outputs = text_generator(
        prompt,
        max_new_tokens=512,      # ìƒì„±í•  ìµœëŒ€ ê¸¸ì´
        do_sample=True,          # ì°½ì˜ì ì¸ ìƒì„±ì„ ìœ„í•´ ìƒ˜í”Œë§ ì‚¬ìš©
        temperature=0.8,         # 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ë³´ìˆ˜ì 
        top_p=0.9,
        repetition_penalty=1.1   # ê°™ì€ ë§ ë°˜ë³µ ë°©ì§€
    )
    
    generated_text = outputs[0]["generated_text"]
    
    # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ìƒì„±ëœ ë‹µë³€ë§Œ ì¶”ì¶œ (ëª¨ë¸ë§ˆë‹¤ ì¶œë ¥ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë‹¨ìˆœí™”)
    # ë³´í†µ <start_of_turn>model ì´í›„ê°€ ë‹µë³€ì…ë‹ˆë‹¤.
    answer = generated_text[len(prompt):]
    
    print("\n[ëª¨ë¸ ìƒì„± ê²°ê³¼]:")
    print(answer.strip())
    print("\n" + "="*50 + "\n")