import torch
from datasets import load_dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, TaskType, get_peft_model
from trl import RewardTrainer, RewardConfig 

# ----------------------------------------------------------------
# 1. ì„¤ì • (Configuration)
# ----------------------------------------------------------------
MODEL_ID = "google/gemma-3-1b-it" 
DATASET_PATH = "./Data/lyrics_rm_data.jsonl"
OUTPUT_DIR = "./Models/reward_model_output"

# 16GB VRAMì„ ìœ„í•œ ìµœì í™” ì„¤ì •
# ê°€ì‚¬ ë°ì´í„° íŠ¹ì„±ìƒ ê¸¸ì´ê°€ ê¸¸ ìˆ˜ ìˆìœ¼ë‚˜, ë©”ëª¨ë¦¬ë¥¼ ìœ„í•´ 1024 ì •ë„ë¡œ ì œí•œ
MAX_LENGTH = 1024  

# ----------------------------------------------------------------
# 2. ë°ì´í„°ì…‹ ë¡œë“œ
# ----------------------------------------------------------------
print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {DATASET_PATH}")
dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (9:1)
dataset = dataset.train_test_split(test_size=0.1)
print(f"í•™ìŠµ ë°ì´í„°: {len(dataset['train'])}ê°œ, ê²€ì¦ ë°ì´í„°: {len(dataset['test'])}ê°œ")

# ----------------------------------------------------------------
# 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (QLoRA ì„¤ì •)
# ----------------------------------------------------------------
print("ğŸ”„ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì¤‘...")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Reward Modelìš© Classification Head (label=1)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_ID,
    num_labels=1, 
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16,
)
model.config.pad_token_id = tokenizer.pad_token_id

# ----------------------------------------------------------------
# 4. LoRA ì„¤ì •
# ----------------------------------------------------------------
peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ----------------------------------------------------------------
# 5. RewardConfig ì„¤ì • (ìµœì‹  TRL ë¬¸ë²• ì ìš©)
# ----------------------------------------------------------------
# ê¸°ì¡´ TrainingArguments ëŒ€ì‹  RewardConfigë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
reward_config = RewardConfig(
    output_dir=OUTPUT_DIR,
    # ë°°ì¹˜ ì‚¬ì´ì¦ˆì™€ Gradient Accumulationìœ¼ë¡œ ë©”ëª¨ë¦¬ ì¡°ì ˆ
    per_device_train_batch_size=1, 
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8, 
    
    learning_rate=1e-4,
    num_train_epochs=1,
    
    # ë©”ëª¨ë¦¬ ì ˆì•½ ì˜µì…˜
    gradient_checkpointing=True,
    fp16=False,
    bf16=True,
    
    # í‰ê°€ ë° ì €ì¥ ì£¼ê¸°
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=100,
    logging_steps=10,
    
    # RewardTrainer ì „ìš© ì¸ì (ì—¬ê¸°ì„œ max_lengthë¥¼ ì§€ì •)
    max_length=MAX_LENGTH,
    center_rewards_coefficient=0.01, # í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•œ ì˜µì…˜ (ì„ íƒì‚¬í•­)
    
    remove_unused_columns=False,
    report_to="none", 
)

# ----------------------------------------------------------------
# 6. RewardTrainer ì‹¤í–‰
# ----------------------------------------------------------------
# tokenizer ì¸ìê°€ 'processing_class'ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.
trainer = RewardTrainer(
    model=model,
    processing_class=tokenizer,  # <--- ë³€ê²½ëœ ë¶€ë¶„
    args=reward_config,          # <--- ë³€ê²½ëœ ë¶€ë¶„
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    peft_config=peft_config,
)

print("ğŸš€ í•™ìŠµ ì‹œì‘! (Reward Model Training - New TRL Version)")
trainer.train()

# ----------------------------------------------------------------
# 7. ì €ì¥
# ----------------------------------------------------------------
print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {OUTPUT_DIR}")
trainer.save_model(OUTPUT_DIR)
print("âœ… Reward Model í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")