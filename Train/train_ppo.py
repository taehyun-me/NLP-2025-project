import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoModelForSequenceClassification,
    BitsAndBytesConfig,
    DataCollatorWithPadding
)
from trl import PPOTrainer, PPOConfig
from peft import LoraConfig, get_peft_model, PeftModel
from datasets import load_dataset
import sys

# ----------------------------------------------------------------
# 1. ì„¤ì • (Configuration)
# ----------------------------------------------------------------
# [ì¤‘ìš”] Reward Model í•™ìŠµ ë•Œ ì‚¬ìš©í–ˆë˜ ëª¨ë¸ê³¼ ë™ì¼í•´ì•¼ í•¨ (ì‚¬ì´ì¦ˆ ë¶ˆì¼ì¹˜ ì—ëŸ¬ í•´ê²°)
MODEL_ID = "google/gemma-3-1b-it" 
RM_ADAPTER_PATH = "./Models/reward_model_output" 

config = PPOConfig(
    exp_name="lyrics_ppo_project",
    learning_rate=1.41e-5,
    
    # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë° ìµœì í™”
    per_device_train_batch_size=1, 
    gradient_accumulation_steps=8,
    
    # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    num_ppo_epochs=1,
    kl_coef=0.05,
    
    # ë©”ëª¨ë¦¬ ì ˆì•½
    gradient_checkpointing=True,
    fp16=False,
    bf16=True, # RTX 5060 Ti ì§€ì›
)

# ----------------------------------------------------------------
# 2. ëª¨ë¸ ë¡œë“œ (ê³µí†µ ì„¤ì •: 4-bit)
# ----------------------------------------------------------------
print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤€ë¹„ ì¤‘... (Policy, Reward, Value)")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token

# ----------------------------------------------------------------
# 3. ê° ëª¨ë¸ ê°œë³„ ë¡œë“œ
# ----------------------------------------------------------------

# [A] Policy Model (Actor)
print("1ï¸âƒ£ Policy Model (Actor) ë¡œë“œ ì¤‘...")
policy_model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)
# ìƒì„± ì„¤ì • ê°•ì œ ì£¼ì… (Trainer ë‚´ë¶€ì—ì„œ ì‚¬ìš©)
policy_model.generation_config.max_new_tokens = 64
policy_model.generation_config.pad_token_id = tokenizer.pad_token_id
policy_model.generation_config.do_sample = True
policy_model.generation_config.top_p = 1.0

policy_peft_config = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05, task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
)
policy_model = get_peft_model(policy_model, policy_peft_config)


# [B] Reward Model (Evaluator)
print("2ï¸âƒ£ Reward Model (Evaluator) ë¡œë“œ ì¤‘...")
reward_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_ID,
    num_labels=1,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)
# í•™ìŠµëœ ì–´ëŒ‘í„° ë¡œë“œ (ì—ëŸ¬ê°€ ë‚˜ë©´ MODEL_IDê°€ ë§ëŠ”ì§€ í™•ì¸ í•„ìˆ˜)
try:
    reward_model = PeftModel.from_pretrained(reward_model, RM_ADAPTER_PATH)
    reward_model.eval() # í•™ìŠµë˜ì§€ ì•Šë„ë¡ ì„¤ì •
    reward_model.requires_grad_(False) # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë”
except Exception as e:
    print(f"âŒ Reward Model ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ğŸ’¡ íŒ: train_reward_model.pyì—ì„œ ì‚¬ìš©í•œ MODEL_IDì™€ í˜„ì¬ MODEL_IDê°€ ê°™ì€ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)


# [C] Value Model (Critic)
print("3ï¸âƒ£ Value Model (Critic) ë¡œë“œ ì¤‘...")
value_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_ID,
    num_labels=1,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)
value_peft_config = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05, task_type="SEQ_CLS",
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
)
value_model = get_peft_model(value_model, value_peft_config)


# ----------------------------------------------------------------
# 4. ë°ì´í„°ì…‹ ì¤€ë¹„ (ìˆ˜ì •ë¨: Train/Eval ë¶„ë¦¬)
# ----------------------------------------------------------------
print("ğŸ“‚ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
raw_dataset = load_dataset("json", data_files="./Data/lyrics_rm_data.jsonl", split="train")

# 1. ë°ì´í„°ì…‹ì„ 9:1ë¡œ ë¶„í•  (Train 90%, Eval 10%)
# ì´ë ‡ê²Œ í•˜ë©´ eval_datasetì´ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.
split_dataset = raw_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

print(f"   - í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
print(f"   - ê²€ì¦ ë°ì´í„°: {len(eval_dataset)}ê°œ")

# 2. í† í¬ë‚˜ì´ì € ì„¤ì •
tokenizer.padding_side = "left" # ìƒì„± ëª¨ë¸ì€ ì™¼ìª½ íŒ¨ë”© í•„ìˆ˜

# 3. ì „ì²˜ë¦¬ í•¨ìˆ˜
def tokenize(sample):
    # max_lengthëŠ” ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ì— ë§ì¶° ì¡°ì ˆ (ì—¬ê¸°ì„  512)
    outputs = tokenizer(sample["prompt"], padding=False, truncation=True, max_length=512)
    return {"input_ids": outputs["input_ids"]}

# 4. ë§¤í•‘ ë° ì»¬ëŸ¼ ì œê±° (Train/Eval ê°ê° ì ìš©)
# remove_columnsë¡œ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì œê±°í•´ì•¼ ì¶©ëŒ ë°©ì§€
train_dataset = train_dataset.map(tokenize, batched=False, remove_columns=["prompt", "chosen", "rejected"])
eval_dataset = eval_dataset.map(tokenize, batched=False, remove_columns=["prompt", "chosen", "rejected"])

# 5. DataCollator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# ----------------------------------------------------------------
# 5. PPOTrainer ì´ˆê¸°í™”
# ----------------------------------------------------------------
print("ğŸš€ PPOTrainer ì´ˆê¸°í™”...")

trainer = PPOTrainer(
    args=config,
    processing_class=tokenizer,
    model=policy_model,
    ref_model=None,
    reward_model=reward_model,
    value_model=value_model,
    train_dataset=train_dataset, # í•™ìŠµìš©
    eval_dataset=eval_dataset,   # ê²€ì¦ìš©
    data_collator=data_collator,
)

print("ğŸ”¥ PPO í•™ìŠµ ì‹œì‘! (ìë™í™”ëœ ë£¨í”„ ì‹¤í–‰)")
trainer.train()